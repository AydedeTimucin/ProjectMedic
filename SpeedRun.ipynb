{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa86e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(image):\n",
    "    # Convert the image to Lab color space\n",
    "    lab_image = image.convert('LAB')\n",
    "\n",
    "    # Split the Lab image into L, a, and b channels\n",
    "    l_channel, a_channel, b_channel = lab_image.split()\n",
    "\n",
    "    # Apply histogram equalization to the L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    equalized_l_channel = clahe.apply(np.array(l_channel))\n",
    "\n",
    "    # Merge the equalized L channel with the original a and b channels\n",
    "    equalized_lab_image = Image.merge('LAB', (Image.fromarray(equalized_l_channel), a_channel, b_channel))\n",
    "\n",
    "    # Convert the equalized Lab image back to RGB\n",
    "    equalized_rgb_image = equalized_lab_image.convert('RGB')\n",
    "\n",
    "    return equalized_rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'H:\\train_patches'\n",
    "\n",
    "# Collect pairs of slide and mask paths\n",
    "train_slide_mask_pairs = []\n",
    "for slide_name in tqdm(os.listdir(root_dir), desc=\"Processing slides\"):\n",
    "    slide_dir = os.path.join(root_dir, slide_name)\n",
    "    \n",
    "    tissue_dir = os.path.join(slide_dir, \"tissues\")\n",
    "    mask_dir = os.path.join(slide_dir, \"masks\")\n",
    "    \n",
    "    for patch_name in os.listdir(tissue_dir):\n",
    "        mask_name = \"mask\" + patch_name[5:]\n",
    "       \n",
    "        patch_path = os.path.join(tissue_dir, patch_name)\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "        \n",
    "        train_slide_mask_pairs.append((patch_path, mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898eae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'H:\\test_patches'\n",
    "\n",
    "# Collect pairs of slide and mask paths\n",
    "test_slide_mask_pairs = []\n",
    "for slide_name in tqdm(os.listdir(root_dir), desc=\"Processing slides\"):\n",
    "    slide_dir = os.path.join(root_dir, slide_name)\n",
    "    \n",
    "    tissue_dir = os.path.join(slide_dir, \"tissues\")\n",
    "    mask_dir = os.path.join(slide_dir, \"masks\")\n",
    "    \n",
    "    for patch_name in os.listdir(tissue_dir):\n",
    "        mask_name = \"mask\" + patch_name[5:]\n",
    "       \n",
    "        patch_path = os.path.join(tissue_dir, patch_name)\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "        \n",
    "        test_slide_mask_pairs.append((patch_path, mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c146755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs, transform):\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tissue_path, mask_path = self.pairs[idx]\n",
    "        \n",
    "        image = Image.open(tissue_path)\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5190fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets for training and testing\n",
    "total_dataset = CustomDataset(pairs=train_slide_mask_pairs, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "total_samples = len(total_dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "valid_size = total_samples - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(total_dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b27a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_samples(dataset, num_samples=5):\n",
    "    for i in range(num_samples):\n",
    "        image, mask = dataset[i]\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "    \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.permute(1, 2, 0).numpy())  # Assuming the image is in the shape (3, 512, 512)\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask[0], cmap='gray')  # Assuming the mask is in the shape (1, 512, 512)\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"\n",
    "# Display samples row by row\n",
    "print(\"Train dataset\")\n",
    "display_samples(train_dataset, num_samples=10)\n",
    "\n",
    "print(\"Valid dataset\")\n",
    "display_samples(valid_dataset, num_samples=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab46d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset.dataset.transform = train_transform\n",
    "valid_dataset.dataset.transform = valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea81bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for train and validation sets\n",
    "bs = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the U-Net architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Create an instance of the UNet model\n",
    "model = UNet()\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "print(criterion)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001, weight_decay=0.0001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize lists to store loss values for plotting\n",
    "train_losses = []\n",
    "train_dice_scores = []\n",
    "valid_losses = []\n",
    "valid_dice_scores = []\n",
    "\n",
    "def dice_coefficient(y_pred, y_true):\n",
    "    intersection = torch.sum(y_true * y_pred)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred)\n",
    "    return (2.0 * intersection) / (union + 1e-8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    total_train_dice = 0.0\n",
    "    for images, masks in  tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\", unit=\"batch\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        train_batch_dice = dice_coefficient(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_dice += train_batch_dice.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    avg_train_dice = total_train_dice / len(train_dataloader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_dice_scores.append(avg_train_dice)\n",
    "\n",
    "    # Validation\n",
    "    \n",
    "    model.eval()  \n",
    "    epoch_valid_loss = 0.0\n",
    "    total_valid_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(valid_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\", unit=\"batch\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate the validation loss\n",
    "            valid_loss = criterion(outputs, masks)\n",
    "            epoch_valid_loss += valid_loss.item()\n",
    "            \n",
    "            # Calculate the Dice coefficient for validation\n",
    "            val_batch_dice = dice_coefficient(outputs, masks)\n",
    "            total_valid_dice += val_batch_dice.item()\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_valid_loss = epoch_valid_loss / len(valid_dataloader)\n",
    "    avg_valid_dice = total_valid_dice / len(valid_dataloader)\n",
    "    \n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    valid_dice_scores.append(avg_valid_dice)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), f\"unet_model_{epoch}.pth\")\n",
    "\n",
    "    # Print losses\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Valid Loss: {avg_valid_loss}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Dice Coeff: {avg_train_dice}\")\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Valid Dice Coeff: {avg_valid_dice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss and Dice coefficient curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_dice_scores, label='Training Dice Coefficient')\n",
    "plt.plot(valid_dice_scores, label='Validation Dice Coefficient')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick for dice\n",
    "\n",
    "best_epoch = np.argmax(valid_dice_scores)\n",
    "print(best_epoch)\n",
    "saved_model_path = f\"unet_model_{best_epoch}.pth\"\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or pick for loss\n",
    "\n",
    "best_epoch = np.argmin(valid_losses)\n",
    "print(best_epoch)\n",
    "saved_model_path = f\"unet_model_{best_epoch}.pth\"\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = CustomDataset(test_slide_mask_pairs, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Initialize lists to store predictions and ground truth masks\n",
    "all_predictions = []\n",
    "all_ground_truths = []\n",
    "\n",
    "# Perform inference\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Store predictions and ground truth mask\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_ground_truths.append(masks.cpu().numpy())\n",
    "        \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

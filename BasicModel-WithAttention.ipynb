{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e917ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93733c99",
   "metadata": {},
   "source": [
    "# DATASET HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root_dir, \"Tissues\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"Masks\", \"binary_mask\")\n",
    "\n",
    "        self.image_files = os.listdir(self.image_dir)\n",
    "        self.mask_files = os.listdir(self.mask_dir)\n",
    "\n",
    "        # Ensure the lists are sorted for matching image-mask pairs\n",
    "        self.image_files.sort()\n",
    "        self.mask_files.sort()\n",
    "\n",
    "        # Check if the number of images and masks match\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Number of images and masks do not match!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        mask_name = self.mask_files[idx]\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomDataset(root_dir=\"H:\\down_scaled_level6_train_processed\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653572ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "valid_size = total_samples - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af8264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_samples(dataset, num_samples=5):\n",
    "    for i in range(num_samples):\n",
    "        image, mask = dataset[i]\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "    \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.permute(1, 2, 0).numpy())  # Assuming the image is in the shape (3, 512, 512)\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask[0], cmap='gray')  # Assuming the mask is in the shape (1, 512, 512)\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "# Display samples row by row\n",
    "print(\"Train dataset\")\n",
    "display_samples(train_dataset, num_samples=10)\n",
    "\n",
    "print(\"Valid dataset\")\n",
    "display_samples(valid_dataset, num_samples=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4031f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset.dataset.transform = train_transform\n",
    "valid_dataset.dataset.transform = valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b303cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for train and validation sets\n",
    "bs = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c5cb0",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f12adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.convolution = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.sigmoid(self.convolution(x))\n",
    "        return x * weights\n",
    "\n",
    "class UNetWithAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetWithAttention, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Attention block\n",
    "        self.attention = AttentionBlock(64)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder(x)\n",
    "\n",
    "        # Attention\n",
    "        x_att = self.attention(x1)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x_att)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Create an instance of the UNetWithAttention model\n",
    "model = UNetWithAttention()\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "print(criterion)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c89334",
   "metadata": {},
   "source": [
    "# MAIN TRAINING AND VALIDATION LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8770a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 40\n",
    "\n",
    "# Initialize lists to store loss values for plotting\n",
    "train_losses = []\n",
    "train_dice_scores = []\n",
    "valid_losses = []\n",
    "valid_dice_scores = []\n",
    "\n",
    "def dice_coefficient(y_pred, y_true):\n",
    "    intersection = torch.sum(y_true * y_pred)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred)\n",
    "    return (2.0 * intersection) / (union + 1e-8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    total_train_dice = 0.0\n",
    "    for images, masks in  tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\", unit=\"batch\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        train_batch_dice = dice_coefficient(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_dice += train_batch_dice.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    avg_train_dice = total_train_dice / len(train_dataloader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_dice_scores.append(avg_train_dice)\n",
    "\n",
    "    # Validation\n",
    "    \n",
    "    model.eval()  \n",
    "    epoch_valid_loss = 0.0\n",
    "    total_valid_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(valid_dataloader, desc=f\"Epoch {epoch}/{num_epochs}\", unit=\"batch\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate the validation loss\n",
    "            valid_loss = criterion(outputs, masks)\n",
    "            epoch_valid_loss += valid_loss.item()\n",
    "            \n",
    "            # Calculate the Dice coefficient for validation\n",
    "            val_batch_dice = dice_coefficient(outputs, masks)\n",
    "            total_valid_dice += val_batch_dice.item()\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_valid_loss = epoch_valid_loss / len(valid_dataloader)\n",
    "    avg_valid_dice = total_valid_dice / len(valid_dataloader)\n",
    "    \n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    valid_dice_scores.append(avg_valid_dice)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), f\"unet_model_{epoch}.pth\")\n",
    "\n",
    "    # Print losses\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Valid Loss: {avg_valid_loss}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Dice Coeff: {avg_train_dice}\")\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Valid Dice Coeff: {avg_valid_dice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98097ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss and Dice coefficient curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_dice_scores, label='Training Dice Coefficient')\n",
    "plt.plot(valid_dice_scores, label='Validation Dice Coefficient')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick for dice\n",
    "\n",
    "best_epoch = np.argmax(valid_dice_scores)\n",
    "print(best_epoch)\n",
    "saved_model_path = f\"unet_model_{best_epoch}.pth\"\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or pick for loss\n",
    "\n",
    "best_epoch = np.argmin(valid_losses)\n",
    "print(best_epoch)\n",
    "saved_model_path = f\"unet_model_{best_epoch}.pth\"\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee75276",
   "metadata": {},
   "source": [
    "# TEST CASE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = CustomDataset(root_dir=\"H:\\down_scaled_level6_test_resized_equalized\", transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Initialize lists to store predictions and ground truth masks\n",
    "all_predictions_equalized = []\n",
    "all_ground_truths = []\n",
    "\n",
    "# Perform inference\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Store predictions and ground truth masks\n",
    "        all_predictions_equalized.append(predictions.cpu().numpy())\n",
    "        all_ground_truths.append(masks.cpu().numpy())\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of thresholds\n",
    "thresholds = np.linspace(0.4, 0.7, 10)  # Adjust the range and step according to your needs\n",
    "\n",
    "best_jaccard_acc = 0.0\n",
    "best_threshold = 0.0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply the threshold to get binary predictions\n",
    "    y_pred_binary = (all_predictions_equalized > threshold).flatten()\n",
    "\n",
    "    # Calculate Jaccard accuracy\n",
    "    jaccard_acc = jaccard_score(y_true_binary, y_pred_binary, average='binary')\n",
    "    print(\"Threshold: \", threshold)\n",
    "    print(\"Accuracy: \", jaccard_acc)\n",
    "    print(\"------------------\")\n",
    "\n",
    "    # Check if the current threshold gives a higher Jaccard accuracy\n",
    "    if jaccard_acc > best_jaccard_acc:\n",
    "        best_jaccard_acc = jaccard_acc\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Jaccard Accuracy: {best_jaccard_acc} at Threshold: {best_threshold}\")\n",
    "\n",
    "all_predictions_equalized = np.concatenate(all_predictions_equalized, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd2ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "num_samples_to_visualize = len(test_dataset)\n",
    "for i in range(num_samples_to_visualize):\n",
    "    # Assuming each element in the dataset is a tuple (image, mask)\n",
    "    sample = test_dataset[i]\n",
    "    \n",
    "    # Convert image and mask tensors to numpy arrays\n",
    "    image = sample[0].numpy()\n",
    "    mask = sample[1].numpy()\n",
    "    \n",
    "    model_prediction = all_predictions_equalized[i, 0] \n",
    "    binary_prediction = (model_prediction > best_threshold).astype(np.uint8)  \n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.transpose((1, 2, 0)))  # Assuming the image is in the shape (3, 512, 512)\n",
    "    plt.title(\"Original Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask[0], cmap='gray')  # Assuming the mask is in the shape (1, 512, 512)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(binary_prediction, cmap='gray')  # Display binary prediction\n",
    "    plt.title(\"Binary Model Prediction\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a0f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
